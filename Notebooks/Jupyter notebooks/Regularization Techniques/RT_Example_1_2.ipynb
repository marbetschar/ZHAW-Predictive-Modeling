{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Methods Example 1.2:\n",
    "In the following, we will discuss the function **sklearn.linear\\_model.Ridge()** in depth. Firstly, note that the qualitative predictors in x have to be transformed into dummy variables. The flag **normalize = True** makes sure that the predictors are mean centred and scaled to unit variance. When comparing to **R**, note that the implementation is slightly different, which makes it hard to compare coefficients as a function of lambda. The optimal solution however, will generally be the same. \n",
    "\n",
    "We will now perform ridge regression in order to predict **Balance** in the **Credit** data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('./data/Credit.csv', index_col=\"Unnamed: 0\")\n",
    "\n",
    "# Convert Categorical variables\n",
    "df = pd.get_dummies(data=df, drop_first=True, \n",
    "                    prefix=('Gender_', 'Student_', \n",
    "                            'Married_', 'Ethnicity_'))\n",
    "\n",
    "# Define target and predictors\n",
    "x = df.drop(columns='Balance') \n",
    "y = df['Balance']\n",
    "\n",
    "# Fit model:\n",
    "lambda_ = 100\n",
    "reg = Ridge(alpha=lambda_, normalize=True)\n",
    "reg = reg.fit(x, y)\n",
    "\n",
    "# Coefficient and corresponding predictors\n",
    "coef = np.round(reg.coef_, 3)\n",
    "# coef = scaler.inverse_transform(coef)\n",
    "x_cols = x.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the coefficient estimates to be much smaller, in terms of $\\ell_2$ norm, when a large value of $\\lambda$ is used, as compared to when a small value of $\\lambda$ is used. These are the coefficients when $\\lambda = 100$, along with their $\\ell_2$ norm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Feature  Coefficient\n",
      "0                 Income        0.006\n",
      "1                  Limit        0.000\n",
      "2                 Rating        0.003\n",
      "3                  Cards        0.029\n",
      "4                    Age        0.000\n",
      "5              Education       -0.001\n",
      "6           Gender__Male       -0.020\n",
      "7           Student__Yes        0.396\n",
      "8           Married__Yes       -0.005\n",
      "9       Ethnicity__Asian       -0.010\n",
      "10  Ethnicity__Caucasian       -0.003 \n",
      "\n",
      "l2-norm: 0.3977901456798547\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(data={'Feature': x_cols,\n",
    "                         'Coefficient':coef}),\n",
    "      '\\n\\nl2-norm:', np.sqrt(np.sum(coef**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, here are the coefficients when $\\lambda = 50$, along with their $\\ell_2$ norm. Note the much larger $\\ell_2$ norm of the coefficients associated with this smaller value of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Feature  Coefficient\n",
      "0                 Income        0.112\n",
      "1                  Limit        0.003\n",
      "2                 Rating        0.049\n",
      "3                  Cards        0.563\n",
      "4                    Age       -0.002\n",
      "5              Education       -0.021\n",
      "6           Gender__Male       -0.379\n",
      "7           Student__Yes        7.773\n",
      "8           Married__Yes       -0.123\n",
      "9       Ethnicity__Asian       -0.183\n",
      "10  Ethnicity__Caucasian       -0.052 \n",
      "\n",
      "l2-norm: 7.806846994786052\n"
     ]
    }
   ],
   "source": [
    "# Fit model:\n",
    "lambda_ = 50\n",
    "reg = Ridge(alpha=lambda_, normalize=True)\n",
    "reg = reg.fit(x, y)\n",
    "\n",
    "# Coefficient and corresponding predictors\n",
    "coef = np.round(reg.coef_, 3)\n",
    "x_cols = x.columns.values\n",
    "\n",
    "print(pd.DataFrame(data={'Feature': x_cols,\n",
    "                         'Coefficient':coef}),\n",
    "      '\\n\\nl2-norm:', np.sqrt(np.sum(coef**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard least squares coefficient estimates are scale equivariant: multiplying a predictor variable $X_j$ by a constant $c$ simply leads to a scaling of the least squares coefficient estimates by a factor of $1/c$. In other words, regardless of how the $j$th predictor is scaled, $\\hat{\\beta}_j X_j $ will remain the\n",
    "same. In contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant. For instance, consider the **income** variable, which is measured in dollars. One could reasonably have measured income in thousands of dollars, which would result in a reduction in the observed values of income by a factor of $1000$. Now due to the sum of squared coefficients in the ridge regression formulation equation, such a change in scale will not simply cause the ridge regression coefficient estimate for **income** to change by a factor of $1000$. In other words,\n",
    "$\\hat{\\beta}_j X_{j,\\lambda}^{R} $ will depend not only on the value of $\\lambda$, but also on the scaling of the $j$th predictor. In fact, the value of $\\hat{\\beta}_j X_{j,\\lambda}^{R} $ may even depend on the scaling of the other predictors. Therefore, it is best to apply ridge regression after standardizing the predictors.\n",
    "\n",
    "Note that by default, the **Ridge()** function does not standardizes the variables. To turn on scaling, use the argument **normalize = True**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
