{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Example 3.1\n",
    "The estimated coefficients are\n",
    "$\\hat{\\beta}_{0} = -10.6513$ \n",
    "\n",
    "$\\hat{\\beta}_{1} = 0.0055$\n",
    "\n",
    "Thus, if an individual has **balance = 1000**, then the model yields\n",
    "\\begin{equation}\n",
    "\\hat{p}(1000)\n",
    "=\\dfrac{e^{-10.65+ 0.0055\\cdot 1000}}{1+e^{-10.65+ 0.0055\\cdot 1000}}\n",
    "\\approx 0.00577\n",
    "\\end{equation}"
   ],
   "id": "4b42aa144ce204a6"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:42:04.414931Z",
     "start_time": "2025-10-16T08:41:53.267572Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.linalg.cython_blas import snrm2\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('./data/Default.csv', sep=';')\n",
    "\n",
    "# Add a numerical column for default\n",
    "df = df.join(pd.get_dummies(df['default'], \n",
    "                            prefix='default', \n",
    "                            drop_first=True))\n",
    "\n",
    "# Fit logistic model\n",
    "x = df['balance']\n",
    "y = df['default_Yes']\n",
    "\n",
    "x_sm = sm.add_constant(x)\n",
    "\n",
    "model = sm.GLM(y, x_sm, family=sm.families.Binomial())\n",
    "model = model.fit()\n",
    "\n",
    "# Predict for balance = 1000\n",
    "x_pred = [1, 1000]\n",
    "y_pred = model.predict(x_pred)\n",
    "\n",
    "print(y_pred)"
   ],
   "id": "c206e481dfccd9f3",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'snrm2' from 'scipy.linalg.cython_blas' (/Users/marbetschar/miniconda3/envs/ZHAW-Predictive-Modeling/lib/python3.13/site-packages/scipy/linalg/cython_blas.cpython-313-darwin.so)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpd\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mstatsmodels\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mapi\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msm\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mscipy\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mlinalg\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcython_blas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m snrm2\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# Load data\u001B[39;00m\n\u001B[32m      7\u001B[39m df = pd.read_csv(\u001B[33m'\u001B[39m\u001B[33m./data/Default.csv\u001B[39m\u001B[33m'\u001B[39m, sep=\u001B[33m'\u001B[39m\u001B[33m;\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[31mImportError\u001B[39m: cannot import name 'snrm2' from 'scipy.linalg.cython_blas' (/Users/marbetschar/miniconda3/envs/ZHAW-Predictive-Modeling/lib/python3.13/site-packages/scipy/linalg/cython_blas.cpython-313-darwin.so)"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This probability of default is well below $1\\%$, which is very low. However, a different individual with **balance = 2000** has a default probability\n",
    "of approximately $59\\%$."
   ],
   "id": "ad0aab0f8cf8161f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Predict for balance = 2000\n",
    "x_pred = [1, 2000]\n",
    "y_pred = model.predict(x_pred)\n",
    "\n",
    "print(y_pred)"
   ],
   "id": "49e0d2d1ef9affe8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Example 3.2\n",
    "For the **Default** data the following **Python**-code computes the training classification error. "
   ],
   "id": "2c2a863a2fb6cec3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\" Follows Example 3.1 \"\"\"\n",
    "# Predict for training data\n",
    "x_pred = x_sm\n",
    "y_pred = model.predict(x_pred)\n",
    "print(y_pred[10])\n",
    "\n",
    "# Round to 0 or 1\n",
    "y_pred = y_pred.round()\n",
    "print(y_pred[10])\n",
    "\n",
    "# Compute training error\n",
    "e_train = abs(y - y_pred)\n",
    "e_train = e_train.mean()\n",
    "\n",
    "print(e_train)"
   ],
   "id": "e9920c98677a8c6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the training error in this example is 0.0275, which is to say that approximately 97.25%  of the cases in the training set are classified correctly. "
   ],
   "id": "777e2764b7e7e9c4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Example 3.3\n",
    "The following **Python**-code produces the confusion matrix for the **Default** data set and the logistic regression model."
   ],
   "id": "3defc94f1e60033c"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\" Follows Example 3.2 \"\"\"\n",
    "# Create confusion matrix\n",
    "confusion = pd.DataFrame({'predicted': y_pred,\n",
    "                          'true': y})\n",
    "confusion = pd.crosstab(confusion.predicted, confusion.true, \n",
    "                        margins=True, margins_name=\"Sum\")\n",
    "\n",
    "print(confusion)"
   ],
   "id": "2fb208f14e697ade",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sn\n",
    "\n",
    "sn.heatmap(confusion, annot=True)"
   ],
   "id": "e1e0e686a752d5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that out of $9667$ cases with **default=No**, the vast majority of $9625$ are classified correctly. On the other hand, only approximately $1/3$\n",
    "of the **default=Yes** cases are classified correctly. The confusion matrix shows that the present classification scheme is by no means useful, in particular,\n",
    "if you want to predict the case of **default=Yes**.\n",
    "\n",
    "The reason for this bad result is the *imbalance* of the two classes. The training data only contains $333$ out of $ 10000$ cases with **default=Yes**.\n",
    "Therefore, the likelihood function is dominated by the factors corresponding to **default=No**, so the parameters are chosen as to match mainly those \n",
    "cases. Note also that the trivial classifier predicting all observations $x$ to $\\hat{f}(x)=0$ has a classification error of $333/10000=0.0333$ which \n",
    "is not much worse than that of our logistic model.\n",
    "\n",
    "The situation can also be visualized by the histograms of the estimated probabilities of **default=Yes** separated by true class.\n",
    "\n",
    "It is striking that the **default=No** group has a high concentration of probabilities near $0$ which is reasonable for this group. On the other hand, though,\n",
    "the estimated probabilities for the **default=Yes** cases do not exhibit high mass at $1$. Instead, the maximal probability is attained close to $0$ as well!"
   ],
   "id": "8868cb4d44dbc6c3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Example 3.8\n",
    "We can compute the F1 score by means of the"
   ],
   "id": "77ba7e5f7dfdfa04"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\" Follows Example 3.3 \"\"\"\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Find F1-score\n",
    "f1 = f1_score(y, y_pred, pos_label=1, average='binary')\n",
    "print(f1)\n"
   ],
   "id": "57f273337dd8cd2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pos\\_label**  is an optional character string for the factor level that corresponds to a *positive* result."
   ],
   "id": "7d3aac02feb1bfb6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Example 3.9\n",
    "If we consider the case **default=No** as positive, then the F1 score changes to"
   ],
   "id": "8514fd6f99d4de35"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\" Follows Example 3.8 \"\"\"\n",
    "# Find F1-score\n",
    "f1 = f1_score(y, y_pred, pos_label=0, average='binary')\n",
    "print(f1)\n"
   ],
   "id": "e5d2dfcd33aa39e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Example 3.10\n",
    "We analyze the **Default** data set and fit a logistic regression model by downsampling the **default=No** class to the same size as the **default=yes** case. "
   ],
   "id": "7f41623684898c95"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\" Follows Example 3.9 \"\"\"\n",
    "# Set ramdom seed\n",
    "np.random.seed(1)\n",
    "# Index of Yes:\n",
    "i_yes = df.loc[df['default_Yes'] == 1, :].index\n",
    "\n",
    "# Random set of No:\n",
    "i_no = df.loc[df['default_Yes'] == 0, :].index\n",
    "i_no = np.random.choice(i_no, replace=False, size=333)\n",
    "\n",
    "# Fit Linear Model on downsampled data\n",
    "i_ds = np.concatenate((i_no, i_yes))\n",
    "x_ds = df.iloc[i_ds]['balance']\n",
    "y_ds = df.iloc[i_ds]['default_Yes']\n",
    "\n",
    "x_sm = sm.add_constant(x_ds)\n",
    "\n",
    "model_ds = sm.GLM(y_ds, x_sm, family=sm.families.Binomial())\n",
    "model_ds = model_ds.fit()\n",
    "\n",
    "# Predict for downsampled data\n",
    "x_pred_ds = x_sm\n",
    "y_pred_ds = model_ds.predict(x_pred_ds)\n",
    "\n",
    "# Round to 0 or 1\n",
    "y_pred_ds = y_pred_ds.round()\n",
    "\n",
    "# Classification error on training data:\n",
    "e_train = abs(y_ds- y_pred_ds)\n",
    "e_train = e_train.mean()\n",
    "\n",
    "print(np.round(e_train, 4))"
   ],
   "id": "ea0c0a9f22150158",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create confusion matrix\n",
    "confusion = pd.DataFrame({'predicted': y_pred_ds,\n",
    "                          'true': y_ds})\n",
    "confusion = pd.crosstab(confusion.predicted, confusion.true, \n",
    "                        margins=True, margins_name=\"Sum\")\n",
    "\n",
    "print(confusion)"
   ],
   "id": "8e145d8b58a7d164",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Print F1-scores\n",
    "f1_pos = f1_score(y_ds, y_pred_ds, pos_label=1, average='binary')\n",
    "f1_neg = f1_score(y_ds, y_pred_ds, pos_label=0, average='binary')\n",
    "\n",
    "print('\\nF1-Score (positive = default) = \\n', f1_pos,\n",
    "      '\\nF1-Score (positive = not-default) = \\n', f1_neg)"
   ],
   "id": "52df8d75541a1166",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the downsampled training set, the confusion matrix is balanced, and the classification error is 0.1171, which amounts to 88.29% correctly classified samples. As we observe now, the F1 score for **default=Yes** as positive case has now considerably improved.\n",
    "\n",
    "Furthermore, the histograms of the predicted probabilities have a complete different shape than before. The separation of the two classes becomes clearly visible."
   ],
   "id": "cdfd8ddcce5891ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
